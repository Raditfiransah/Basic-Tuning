{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ec0aa5",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Kita udah nyediain array NumPy di bawah ini. Atribut dtype ngasih tau kita tipe data dari array tersebut. Atribut itemsize ngasih tau ukuran tiap item di array dalam byte.\n",
    "\n",
    "Jalanin cell di bawah buat ngecek hasil output-nya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ce4404d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type:  float32\n",
      "Bytes needed per element:  4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "numbers = np.array([1.23456, 2.3456, 3.456, 7.56, 15.6745], dtype = 'float32')\n",
    "print(\"Data type: \", numbers.dtype)\n",
    "print(\"Bytes needed per element: \", numbers.itemsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd250086",
   "metadata": {},
   "source": [
    "## Mengubah Tipe Data di NumPy\n",
    "\n",
    "Kita udah nulis kode di bawah ini buat ngubah angka-angka jadi array tipe FP16 pake fungsi np.float16. Jalanin cell berikut buat lihat gimana perubahan angka-angka tersebut."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9984fc",
   "metadata": {},
   "source": [
    "## Float 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dce57c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.234  2.346  3.455  7.56  15.67 ]\n",
      "Bytes needed per element:  2\n"
     ]
    }
   ],
   "source": [
    "f16_numbers = np.float16(numbers)\n",
    "f16_bytes_per_element = f16_numbers.itemsize\n",
    "print(f16_numbers)\n",
    "print(\"Bytes needed per element: \", f16_bytes_per_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c5046d",
   "metadata": {},
   "source": [
    "## Integer 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fce269e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  7 15]\n",
      "Bytes needed per element:  1\n"
     ]
    }
   ],
   "source": [
    "### YOUR SOLUTION HERE ###\n",
    "int8_numbers = np.int8(numbers)\n",
    "int8_bytes_per_element = int8_numbers.itemsize\n",
    "print(int8_numbers)\n",
    "print(\"Bytes needed per element: \", int8_bytes_per_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a333ab7",
   "metadata": {},
   "source": [
    "## Quantization in Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c9fd62",
   "metadata": {},
   "source": [
    "Asumsi Skenario:\n",
    "\n",
    "Sekarang kita bakal anggap array numbers dari checkpoint sebelumnya itu isinya weights dari model deep learning. Dan kita bakal nyobain fungsi simple buat quantize array itu.\n",
    "\n",
    "Metode yang tadi sebenernya udah ngubah nilai di array jadi integer, tapi masih bisa dibikin lebih bagus. Soalnya cara itu cuma pembulatan ke bawah, jadi informasi dari array aslinya bisa ilang. Contohnya nih: kalau semua weight nilainya < 1, terus dikonversi ke np.int8, semuanya jadi 0 dong! ðŸ˜­\n",
    "\n",
    "Biar hasilnya lebih akurat dan gak ngawur, kita perlu memperhitungkan range dari nilai di array aslinya dibandingin sama range angka yang bisa diwakilin oleh tipe data tujuan.\n",
    "\n",
    "Solusinya:\n",
    "\n",
    "Kita udah nulis fungsi simple buat quantize array pake metode yang disebut Absmax Quantization. Fungsi ini udah siap dipake di cell berikut.\n",
    "\n",
    "Jalanin aja cell-nya buat nge-define fungsinya dulu ðŸ’ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5178b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(x):\n",
    "    # scaling the values\n",
    "    scale = 127 / max(abs(x))\n",
    "    # quantizing\n",
    "    quantized_value = (scale * x).round()\n",
    "    return quantized_value.astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "881a60cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10  19  28  61 127]\n"
     ]
    }
   ],
   "source": [
    "quantized_array = quantize(numbers)\n",
    "print(quantized_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00394997",
   "metadata": {},
   "source": [
    "## Quantization di Hugging Face\n",
    "\n",
    "Kita barusan berhasil quantize array dari float32 (yang butuh 4 byte per elemen) jadi int8 (yang cuma butuh 1 byte per elemen)! ðŸ”¥\n",
    "\n",
    "Sekarang coba bayangin kalo kita ngelakuin hal yang sama ke model transformer gede, yang punya bobot (weight matrix) segede gaban. Seberapa banyak memori komputasi model itu bisa dipangkas? ðŸ¤¯\n",
    "\n",
    "Untungnya, kita nggak perlu overthinking karena Hugging Face udah nyediain solusi praktis lewat library bitsandbytes. Dokumentasinya bisa lo cek di sini:\n",
    "[https://huggingface.co/docs/bitsandbytes/main/en/index](https://huggingface.co/docs/bitsandbytes/main/en/index)\n",
    "\n",
    "Kita juga udah nyiapin kode buat ngitung ukuran memori model distilgpt2 â€” versi yang lebih ramping dari GPT-2. Langsung aja jalanin cell-nya buat liat seberapa besar ukuran modelnya dalam satuan byte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdb8d37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Radit\\anaconda3\\envs\\environment\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m torch.manual_seed(\u001b[32m19\u001b[39m)\n\u001b[32m      5\u001b[39m model_id = \u001b[33m'\u001b[39m\u001b[33mdistilgpt2\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "torch.manual_seed(19)\n",
    "\n",
    "model_id = 'distilgpt2'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "print(\"Model size in bytes: \", model.get_memory_footprint())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77baeace",
   "metadata": {},
   "source": [
    "Tambahin dua argumen lagi ke metode from_pretrained biar kita bisa nge-load versi 8-bit quantized dari model distilgpt2:\n",
    "\n",
    "Set device_map ke 'auto'\n",
    "\n",
    "Set load_in_8bit ke True\n",
    "\n",
    "Terus jangan lupa buat uncomment baris yang nge-print ukuran model-nya, dan jalanin cell-nya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4236b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_quantized_model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                            ### YOUR SOLUTION HERE ###\n",
    "                                                            #device_map=  ,\n",
    "                                                            #\n",
    "                                             )\n",
    "#print(\"8-bit Quantized model size in bytes: \", int8_quantized_model.get_memory_footprint())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
